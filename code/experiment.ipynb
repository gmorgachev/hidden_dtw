{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from time import time\n",
    "from numpy.random import choice, shuffle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from dtaidistance import dtw, dtw_c, dtw_ndim\n",
    "\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from preprocessing import *\n",
    "from models import Encoder, Decoder, Sequence2Sequence\n",
    "\n",
    "%aimport preprocessing\n",
    "%aimport models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def get_dataset(data_path, length=100):\n",
    "    df_train = pd.read_csv(data_path + TRAIN + \".txt\", header=None, delim_whitespace=True)\n",
    "    df_test = pd.read_csv(data_path + TEST + \".txt\", header=None, delim_whitespace=True)\n",
    "        \n",
    "    df = pd.concat([df_test, df_train], ignore_index=True, sort=False)\n",
    "    X = df.iloc[:, 1:].values\n",
    "    y = df.iloc[:, 0].values\n",
    "\n",
    "    if np.isnan(X).any():\n",
    "        timeseries = []\n",
    "        labels = []\n",
    "        for label, x in zip(y, X):\n",
    "            ts = slice_timeseries(x[~np.isnan(x)], 100)\n",
    "            timeseries.extend(ts)\n",
    "            labels.extend(np.repeat(label, len(ts)))\n",
    "\n",
    "        X = np.asarray(timeseries)\n",
    "        y = np.asarray(labels)\n",
    "    else:\n",
    "        X = slice_timeseries(X.swapaxes(1, 0), min(length, X.shape[-1])).swapaxes(1, 2)\n",
    "        X = np.vstack(X)\n",
    "        y = np.repeat(y, X.shape[0] // y.shape[0])\n",
    "    \n",
    "    assert not np.isnan(X).any(), \"Stand from under! NAN in data!\"\n",
    "\n",
    "    return zscore(X, 1), y\n",
    "\n",
    "\n",
    "def prepare_data(X, y, k, w):\n",
    "    ds = SplittedDataset(X, y, k, w, device=device)\n",
    "    train_ds, test_ds, valid_ds = train_test_valid_split(ds, 0.05, 0.75)\n",
    "\n",
    "    train_set = DataLoader(train_ds, batch_size=1024, shuffle=True)\n",
    "    test_set = DataLoader(test_ds, batch_size=1024, shuffle=True)\n",
    "    valid_set = DataLoader(valid_ds, batch_size=256, shuffle=True)\n",
    "    \n",
    "    return train_set, test_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(hidden_dim, k, n_layers):\n",
    "    input_dim = 2*k\n",
    "    enc = Encoder(input_dim, hidden_dim, 1, n_layers, False)\n",
    "    dec = Decoder(hidden_dim, input_dim, 1, n_layers)\n",
    "\n",
    "    model = Sequence2Sequence(enc, dec)\n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(model, train_ds, optim, loss_fn, valid_ds, n_step):\n",
    "    model.train()\n",
    "    for step in range(n_step):\n",
    "        it = iter(train_ds)\n",
    "        train_loss = 0.\n",
    "        for batch, _, _ in it:\n",
    "            batch = batch.permute(1, 0, 2)\n",
    "            optim.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = loss_fn(batch, out)\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "        if (step+1) % 500 == 0:\n",
    "            valid_loss = valid(model, valid_ds, loss_fn)\n",
    "            print(\"{:4d}: train loss: {:.3f}; valid loss: {:.3f}\".format(step+1, train_loss, valid_loss))\n",
    "            model.train()\n",
    "\n",
    "\n",
    "def valid(model, valid_ds, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0.\n",
    "        it = iter(valid_ds)\n",
    "        for batch, _, _ in it:\n",
    "            batch = batch.permute(1, 0, 2)\n",
    "            out = model(batch)\n",
    "            loss += loss_fn(batch, out)\n",
    "    \n",
    "    return loss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metric(id1, id2, matrix):\n",
    "    \"\"\"\n",
    "    Return distance between elements from distance matrix\n",
    "    \"\"\"\n",
    "    x, y = int(min(id1[0], id2[0])), int(max(id1[0], id2[0]))\n",
    "    return matrix[x, y]\n",
    "\n",
    "\n",
    "def get_metric(matrix):\n",
    "    \"\"\"\n",
    "    Return distance function from distance matrix\n",
    "    \"\"\"\n",
    "    return lambda id1, id2: _metric(id1, id2, matrix)\n",
    "\n",
    "\n",
    "def workflow(w, k, hidden_dim, n_layers, length, n_step=6000):\n",
    "    \"\"\"\n",
    "    Plan of the experiment\n",
    "    \"\"\"\n",
    "    X, y = get_dataset(length)\n",
    "    train_set, test_set, valid_set = prepare_data(X, y, k, w)\n",
    "    model = get_model(hidden_dim, k, n_layers)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    train(model, train_set, optim, loss_fn, test_set, n_step)\n",
    "    torch.save(model.state_dict(), \"../data/w={};k={};nl={};length={};h={}\".format(w, k, n_layers, length, hidden_dim))\n",
    "    valid(model, test_set, loss_fn)\n",
    "    valid_it = iter(valid_set)\n",
    "    batch, timeseries, labels = next(valid_it)\n",
    "    timeseries = timeseries.numpy()\n",
    "\n",
    "    scores_ts = []\n",
    "    scores_hidden = []\n",
    "\n",
    "    t = time()\n",
    "    matrix_ts = dtw.distance_matrix(timeseries, use_c=True)\n",
    "    print(\"raw_ts: {:.3f}\".format(time() - t))\n",
    "    t = time()\n",
    "    hiddens = model.encoder(batch.permute(1, 0, 2))[0].permute(1, 0, 2).detach().cpu().numpy()\n",
    "    matrix_hidden = dtw_ndim.distance_matrix(hiddens)\n",
    "    print(\"hidden_ts: {:.3f}\".format(time() - t))\n",
    "    idxs = np.arange(len(timeseries)).reshape(-1, 1)\n",
    "\n",
    "    for i in range(100):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(idxs, labels.cpu().numpy(), test_size=0.4)\n",
    "\n",
    "        clf = KNeighborsClassifier(metric=get_metric(matrix_ts), algorithm=\"brute\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        scores_ts.append(score)\n",
    "        clf = KNeighborsClassifier(metric=get_metric(matrix_hidden), algorithm=\"brute\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        scores_hidden.append(score)\n",
    "\n",
    "    print(\"Raw ts score: {:.3f} +- {:.3f}\".format(np.mean(scores_ts), np.std(scores_ts)))    \n",
    "    print(\"Hidden ts score: {:.3f} +- {:.3f}\".format(np.mean(scores_hidden), np.std(scores_hidden)))\n",
    "    \n",
    "    return model, train_set, test_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, path\n",
    "from arff2pandas import a2p\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |       Problem        | Items | Length\n",
      "-------------------------------------------\n",
      "  0 |                ACSF1 |  2800 |    100\n",
      "hidden_ts: 170.316\n",
      "raw_ts: 259.027\n",
      "Raw ts score: 0.576 +- 0.046\n",
      "Hidden ts score: 0.570 +- 0.043\n",
      "  1 |                Adiac |   781 |    100\n",
      "hidden_ts: 167.534\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"../data/Univariate_arff\"\n",
    "TRAIN = \"_TRAIN\"\n",
    "TEST = \"_TEST\"\n",
    "\n",
    "datasets = listdir(BASE_PATH)\n",
    "w = 3\n",
    "k = 4\n",
    "hidden_dim = 3\n",
    "length = 100\n",
    "sample_size = 220\n",
    "\n",
    "header = \"{0:3s} | {1:^20s} | {2:^4s} | {3:^4s}\".format(\"\", \"Problem\", \"Items\", \"Length\")\n",
    "print(header)\n",
    "print(\"-\"*len(header))\n",
    "for idx, problem in enumerate(datasets):    \n",
    "    data_path = path.join(BASE_PATH, problem, problem)    \n",
    "    pca = PCA(n_components=hidden_dim)\n",
    "    X, y = get_dataset(data_path)\n",
    "    print(\"{0:3d} | {1:>20s} | {2:5d} | {3:6d}\".format(\n",
    "        idx, problem[:20], X.shape[0], X.shape[1]))\n",
    "    X = X[:sample_size]\n",
    "    y = y[:sample_size]\n",
    "    train_set, test_set, valid_set = prepare_data(X, y, k, w)\n",
    "    train_it = iter(train_set)\n",
    "    batch, timeseries, labels = next(train_it)\n",
    "    batch = batch.cpu().detach().numpy()\n",
    "    for x in batch:\n",
    "        pca = pca.fit(x)\n",
    "    \n",
    "    valid_it = iter(valid_set)\n",
    "    batch, timeseries, labels = next(valid_it)\n",
    "    batch = batch.cpu().detach().numpy()\n",
    "    timeseries = timeseries.numpy()\n",
    "    idxs = np.arange(len(timeseries)).reshape(-1, 1)\n",
    "\n",
    "    scores_hidden = []\n",
    "    scores_ts = []\n",
    "    t = time()\n",
    "    hiddens = np.stack([pca.transform(x) for x in batch])\n",
    "    matrix_hidden = dtw_ndim.distance_matrix(hiddens)\n",
    "    print(\"hidden_ts: {:.3f}\".format(time() - t))\n",
    "    \n",
    "    t = time()\n",
    "    matrix_ts = dtw.distance_matrix(timeseries, use_c=True)\n",
    "    print(\"raw_ts: {:.3f}\".format(time() - t))\n",
    "    \n",
    "    for i in range(100):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(idxs, labels.cpu().numpy(), test_size=0.7)\n",
    "        clf = KNeighborsClassifier(metric=get_metric(matrix_ts), algorithm=\"brute\", n_neighbors=3)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        scores_ts.append(score)\n",
    "        clf = KNeighborsClassifier(metric=get_metric(matrix_hidden), algorithm=\"brute\", n_neighbors=3)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        scores_hidden.append(score)\n",
    "    \n",
    "    print(\"Raw ts score: {:.3f} +- {:.3f}\".format(np.mean(scores_ts), np.std(scores_ts)))    \n",
    "    print(\"Hidden ts score: {:.3f} +- {:.3f}\".format(np.mean(scores_hidden), np.std(scores_hidden)))\n",
    "\n",
    "    \n",
    "\n",
    "# problem = datasets[88]\n",
    "# data_path = path.join(BASE_PATH, problem, problem)\n",
    "# X, y = get_dataset(data_path)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.swapaxes(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hidden_dtw",
   "language": "python",
   "name": "hidden_dtw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
