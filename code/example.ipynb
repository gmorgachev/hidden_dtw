{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from dtaidistance import dtw, dtw_c, dtw_ndim\n",
    "from numpy.random import choice, shuffle\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "from time import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from preprocessing import *\n",
    "from models import Encoder, Decoder, Sequence2Sequence\n",
    "\n",
    "%aimport preprocessing\n",
    "%aimport models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of multidimensional time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p data\n",
    "# !wget https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/UCRArchive_2018.zip ../data\n",
    "# !unzip data/UCRArchive_2018.zip -d ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"../data/UCRArchive_2018/Earthquakes/Earthquakes_TRAIN.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(data_path, header=None, delimiter=\"\\t\").values\n",
    "\n",
    "# # to remove Nan (only first columns)\n",
    "# data = data[:, ~np.isnan(data).any(0)]\n",
    "\n",
    "# X = data[:, 1:]\n",
    "# y = data[:, 0].astype(np.short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B_Accelerometer preprocessing\n",
    "\n",
    "[Link](https://github.com/mmalekzadeh/motion-sense/blob/master/data/B_Accelerometer_data.zip \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_2(length):\n",
    "    LABELS = [0, 1, 4, 5]\n",
    "\n",
    "    labels = np.unique([x[:3] for x in os.listdir(\"../data/B_Accelerometer_data/\")])\n",
    "    dirs = os.listdir(\"../data/B_Accelerometer_data/\")\n",
    "    d = dict((label, i) for (i, label) in enumerate(labels))\n",
    "\n",
    "    timeseries = {label: [] for label in d.values()}\n",
    "\n",
    "    for dir_name in dirs:\n",
    "        tmp = []\n",
    "        for f in os.listdir(\"../data/B_Accelerometer_data/{}\".format(dir_name)):\n",
    "            data = pd.read_csv(\"../data/B_Accelerometer_data/{}/{}\".format(dir_name, f), index_col=0)\n",
    "\n",
    "            tmp.append(np.linalg.norm(data, axis=1))\n",
    "\n",
    "        timeseries[d[dir_name[:3]]] = zscore(np.hstack(tmp))\n",
    "\n",
    "    dataset = {label: [] for label in LABELS}\n",
    "    for label in LABELS:\n",
    "        X = timeseries[label][np.newaxis].T\n",
    "        dataset[label].extend(slice_timeseries(X, length))\n",
    "\n",
    "\n",
    "    max_count = min([len(x) for x in dataset.values()])\n",
    "    for label, ts in dataset.items():\n",
    "        dataset[label] = [ts[idx] for idx in choice(range(len(ts)), max_count, replace=False)]\n",
    "\n",
    "    X = np.array([zscore(x) for _, ts in dataset.items() for x in ts])\n",
    "    y = [np.repeat(label, len(ts)) for label, ts in dataset.items()]\n",
    "    X = np.hstack(X).T\n",
    "    y = np.hstack(y).T\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(len(LABELS), figsize=(10, 15))\n",
    "\n",
    "# for i, label in enumerate(LABELS):\n",
    "#     axs[i].plot(dataset[label][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerometer dataset \n",
    "\n",
    "- 1: Working at Computer\n",
    "- 2: Standing Up, Walking and Going up\\down stairs\n",
    "- 3: Standing\n",
    "- 4: Walking\n",
    "- 5: Going Up\\Down Stairs\n",
    "- 6: Walking and Talking with Someone\n",
    "- 7: Talking while Standing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00287/Activity%20Recognition%20from%20Single%20Chest-Mounted%20Accelerometer.zip -O ../data/Accelerometer.zip\n",
    "# ! unzip ../data/Accelerometer.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = ['0', '1', '2']\n",
    "LABELS = [1, 3, 4, 5]\n",
    "\n",
    "LENGTH = 100\n",
    "start_ident = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_3():\n",
    "    np.random.seed(0)\n",
    "\n",
    "\n",
    "    dataset = {label: [] for label in LABELS}\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        data_path = \"../data/Activity Recognition from Single Chest-Mounted Accelerometer/{0}.csv\".format(i)\n",
    "        data = pd.read_csv(data_path,\n",
    "                       names=['0', '1', '2', \"labels\"],\n",
    "                       index_col=0,\n",
    "                       dtype=np.float)\n",
    "\n",
    "        for label in LABELS:\n",
    "            X = get_class_timeseries(label, data, start_ident)\n",
    "            X = np.linalg.norm(X, axis=1)[np.newaxis].T\n",
    "            dataset[label].extend(slice_timeseries(X, LENGTH))\n",
    "\n",
    "    max_count = min([len(x) for x in dataset.values()])\n",
    "    for label, ts in dataset.items():\n",
    "        dataset[label] = [ts[idx] for idx in choice(range(len(ts)), max_count, replace=False)]\n",
    "\n",
    "    X = np.array([zscore(x) for _, ts in dataset.items() for x in ts])\n",
    "    y = [np.repeat(label, len(ts)) for label, ts in dataset.items()]\n",
    "    X = np.hstack(X).T\n",
    "    y = np.hstack(y).T\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def prepare_data(X, y, k, w):\n",
    "    ds = SplittedDataset(X, y, k, w, device=device)\n",
    "    train_ds, test_ds, valid_ds = train_test_valid_split(ds, 0.3, 0.2)\n",
    "\n",
    "    train_set = DataLoader(train_ds, batch_size=1024, shuffle=True)\n",
    "    test_set = DataLoader(test_ds, batch_size=1024, shuffle=True)\n",
    "    valid_set = DataLoader(valid_ds, batch_size=256, shuffle=True)\n",
    "    \n",
    "    return train_set, test_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(hidden_dim, k, n_layers):\n",
    "    input_dim = 2*k\n",
    "    enc = Encoder(input_dim, hidden_dim, 1, n_layers, False)\n",
    "    dec = Decoder(hidden_dim, input_dim, 1, n_layers)\n",
    "\n",
    "    model = Sequence2Sequence(enc, dec)\n",
    "    model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def train(model, train_ds, optim, loss_fn, valid_ds, n_step):\n",
    "    model.train()\n",
    "    for step in range(n_step):\n",
    "        it = iter(train_ds)\n",
    "        train_loss = 0.\n",
    "        for batch, _, _ in it:\n",
    "            batch = batch.permute(1, 0, 2)\n",
    "            optim.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = loss_fn(batch, out)\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "        if (step+1) % 500 == 0:\n",
    "            valid_loss = valid(model, valid_ds, loss_fn)\n",
    "            print(\"{:4d}: train loss: {:.3f}; valid loss: {:.3f}\".format(step+1, train_loss, valid_loss))\n",
    "            model.train()\n",
    "\n",
    "\n",
    "def valid(model, valid_ds, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0.\n",
    "        it = iter(valid_ds)\n",
    "        for batch, _, _ in it:\n",
    "            batch = batch.permute(1, 0, 2)\n",
    "            out = model(batch)\n",
    "            loss += loss_fn(batch, out)\n",
    "    \n",
    "    return loss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(id1, id2, matrix):\n",
    "    x, y = int(min(id1[0], id2[0])), int(max(id1[0], id2[0]))\n",
    "    return matrix[x, y]\n",
    "\n",
    "def get_metric(matrix):\n",
    "    return lambda id1, id2: m(id1, id2, matrix)\n",
    "\n",
    "def workflow(w, k, hidden_dim, n_layers, length, n_step=6000):\n",
    "    X, y = get_dataset_2(length)\n",
    "    train_set, test_set, valid_set = prepare_data(X, y, k, w)\n",
    "    model = get_model(hidden_dim, k, n_layers)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    train(model, train_set, optim, loss_fn, test_set, n_step)\n",
    "    torch.save(model.state_dict(), \"../data/w={};k={};nl={};length={}\".format(w, k, n_layers, length))\n",
    "    valid(model, test_set, loss_fn)\n",
    "    valid_it = iter(valid_set)\n",
    "    batch, timeseries, labels = next(valid_it)\n",
    "    timeseries = timeseries.numpy()\n",
    "\n",
    "    scores_ts = []\n",
    "    scores_hidden = []\n",
    "\n",
    "    t = time()\n",
    "    matrix_ts = dtw.distance_matrix(timeseries, use_c=True)\n",
    "    print(\"raw_ts: {:.3f}\".format(time() - t))\n",
    "    t = time()\n",
    "    hiddens = model.encoder(batch.permute(1, 0, 2))[0].permute(1, 0, 2).detach().cpu().numpy()\n",
    "    matrix_hidden = dtw_ndim.distance_matrix(hiddens)\n",
    "    print(\"hidden_ts: {:.3f}\".format(time() - t))\n",
    "    idxs = np.arange(len(timeseries)).reshape(-1, 1)\n",
    "\n",
    "    for i in range(100):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(idxs, labels.cpu().numpy(), test_size=0.4)\n",
    "\n",
    "        clf = KNeighborsClassifier(metric=get_metric(matrix_ts), algorithm=\"brute\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        scores_ts.append(score)\n",
    "        clf = KNeighborsClassifier(metric=get_metric(matrix_hidden), algorithm=\"brute\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        scores_hidden.append(score)\n",
    "\n",
    "    print(\"Raw ts score: {:.3f} +- {:.3f}\".format(np.mean(scores_ts), np.std(scores_ts)))    \n",
    "    print(\"Hidden ts score: {:.3f} +- {:.3f}\".format(np.mean(scores_hidden), np.std(scores_hidden)))\n",
    "    \n",
    "    return scores_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################### w=3; k=3; hidden=4; nl=1 ###########################\n",
      " 500: train loss: 0.832; valid loss: 0.418\n",
      "1000: train loss: 0.705; valid loss: 0.353\n",
      "1500: train loss: 0.661; valid loss: 0.333\n",
      "2000: train loss: 0.647; valid loss: 0.322\n",
      "raw_ts: 607.128\n",
      "hidden_ts: 453.845\n",
      "Raw ts score: 0.680 +- 0.039\n",
      "Hidden ts score: 0.704 +- 0.042\n",
      "########################### w=3; k=3; hidden=4; nl=2 ###########################\n",
      " 500: train loss: 0.830; valid loss: 0.409\n",
      "1000: train loss: 0.788; valid loss: 0.392\n",
      "1500: train loss: 0.769; valid loss: 0.382\n",
      "2000: train loss: 0.662; valid loss: 0.325\n",
      "raw_ts: 608.901\n",
      "hidden_ts: 444.392\n",
      "Raw ts score: 0.679 +- 0.043\n",
      "Hidden ts score: 0.622 +- 0.043\n",
      "########################### w=3; k=3; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 0.851; valid loss: 0.419\n",
      "1000: train loss: 0.785; valid loss: 0.392\n",
      "1500: train loss: 0.769; valid loss: 0.378\n",
      "2000: train loss: 0.744; valid loss: 0.369\n",
      "raw_ts: 604.930\n",
      "hidden_ts: 476.735\n",
      "Raw ts score: 0.710 +- 0.042\n",
      "Hidden ts score: 0.618 +- 0.045\n",
      "########################### w=3; k=3; hidden=6; nl=1 ###########################\n",
      " 500: train loss: 0.745; valid loss: 0.366\n",
      "1000: train loss: 0.667; valid loss: 0.326\n",
      "1500: train loss: 0.638; valid loss: 0.314\n",
      "2000: train loss: 0.629; valid loss: 0.308\n",
      "raw_ts: 621.887\n",
      "hidden_ts: 453.395\n",
      "Raw ts score: 0.694 +- 0.040\n",
      "Hidden ts score: 0.754 +- 0.039\n",
      "########################### w=3; k=3; hidden=6; nl=2 ###########################\n",
      " 500: train loss: 0.828; valid loss: 0.405\n",
      "1000: train loss: 0.692; valid loss: 0.343\n",
      "1500: train loss: 0.639; valid loss: 0.320\n",
      "2000: train loss: 0.626; valid loss: 0.313\n",
      "raw_ts: 612.035\n",
      "hidden_ts: 446.604\n",
      "Raw ts score: 0.725 +- 0.037\n",
      "Hidden ts score: 0.697 +- 0.041\n",
      "########################### w=3; k=3; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 0.833; valid loss: 0.414\n",
      "1000: train loss: 0.789; valid loss: 0.396\n",
      "1500: train loss: 0.762; valid loss: 0.387\n",
      "2000: train loss: 0.750; valid loss: 0.380\n",
      "raw_ts: 639.624\n",
      "hidden_ts: 464.591\n",
      "Raw ts score: 0.682 +- 0.045\n",
      "Hidden ts score: 0.640 +- 0.047\n",
      "########################### w=3; k=5; hidden=4; nl=1 ###########################\n",
      " 500: train loss: 0.832; valid loss: 0.413\n",
      "1000: train loss: 0.736; valid loss: 0.368\n",
      "1500: train loss: 0.709; valid loss: 0.355\n",
      "2000: train loss: 0.704; valid loss: 0.347\n",
      "raw_ts: 629.467\n",
      "hidden_ts: 427.997\n",
      "Raw ts score: 0.651 +- 0.038\n",
      "Hidden ts score: 0.706 +- 0.037\n",
      "########################### w=3; k=5; hidden=4; nl=2 ###########################\n",
      " 500: train loss: 0.859; valid loss: 0.425\n",
      "1000: train loss: 0.814; valid loss: 0.403\n",
      "1500: train loss: 0.701; valid loss: 0.349\n",
      "2000: train loss: 0.677; valid loss: 0.337\n",
      "raw_ts: 629.967\n",
      "hidden_ts: 467.651\n",
      "Raw ts score: 0.732 +- 0.036\n",
      "Hidden ts score: 0.697 +- 0.042\n",
      "########################### w=3; k=5; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 0.836; valid loss: 0.426\n",
      "1000: train loss: 0.777; valid loss: 0.403\n",
      "1500: train loss: 0.763; valid loss: 0.392\n",
      "2000: train loss: 0.749; valid loss: 0.385\n",
      "raw_ts: 720.394\n",
      "hidden_ts: 478.132\n",
      "Raw ts score: 0.657 +- 0.041\n",
      "Hidden ts score: 0.656 +- 0.042\n",
      "########################### w=3; k=5; hidden=6; nl=1 ###########################\n",
      " 500: train loss: 0.795; valid loss: 0.392\n",
      "1000: train loss: 0.701; valid loss: 0.346\n",
      "1500: train loss: 0.667; valid loss: 0.332\n",
      "2000: train loss: 0.655; valid loss: 0.324\n",
      "raw_ts: 626.232\n",
      "hidden_ts: 483.166\n",
      "Raw ts score: 0.665 +- 0.045\n",
      "Hidden ts score: 0.702 +- 0.047\n",
      "########################### w=3; k=5; hidden=6; nl=2 ###########################\n",
      " 500: train loss: 0.806; valid loss: 0.411\n",
      "1000: train loss: 0.700; valid loss: 0.355\n",
      "1500: train loss: 0.669; valid loss: 0.336\n",
      "2000: train loss: 0.639; valid loss: 0.327\n",
      "raw_ts: 646.386\n",
      "hidden_ts: 497.316\n",
      "Raw ts score: 0.688 +- 0.038\n",
      "Hidden ts score: 0.730 +- 0.042\n",
      "########################### w=3; k=5; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 0.855; valid loss: 0.430\n",
      "1000: train loss: 0.804; valid loss: 0.404\n",
      "1500: train loss: 0.786; valid loss: 0.394\n",
      "2000: train loss: 0.767; valid loss: 0.387\n",
      "raw_ts: 656.985\n",
      "hidden_ts: 425.194\n",
      "Raw ts score: 0.669 +- 0.043\n",
      "Hidden ts score: 0.664 +- 0.040\n",
      "########################### w=3; k=7; hidden=4; nl=1 ###########################\n",
      " 500: train loss: 0.891; valid loss: 0.441\n",
      "1000: train loss: 0.785; valid loss: 0.383\n",
      "1500: train loss: 0.742; valid loss: 0.364\n",
      "2000: train loss: 0.724; valid loss: 0.355\n",
      "raw_ts: 598.291\n",
      "hidden_ts: 372.363\n",
      "Raw ts score: 0.680 +- 0.049\n",
      "Hidden ts score: 0.723 +- 0.037\n",
      "########################### w=3; k=7; hidden=4; nl=2 ###########################\n",
      " 500: train loss: 0.826; valid loss: 0.406\n",
      "1000: train loss: 0.740; valid loss: 0.367\n",
      "1500: train loss: 0.707; valid loss: 0.351\n",
      "2000: train loss: 0.685; valid loss: 0.342\n",
      "raw_ts: 594.031\n",
      "hidden_ts: 378.977\n",
      "Raw ts score: 0.634 +- 0.040\n",
      "Hidden ts score: 0.709 +- 0.042\n",
      "########################### w=3; k=7; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 0.851; valid loss: 0.425\n",
      "1000: train loss: 0.829; valid loss: 0.407\n",
      "1500: train loss: 0.803; valid loss: 0.399\n",
      "2000: train loss: 0.784; valid loss: 0.392\n",
      "raw_ts: 618.163\n",
      "hidden_ts: 363.215\n",
      "Raw ts score: 0.688 +- 0.035\n",
      "Hidden ts score: 0.634 +- 0.042\n",
      "########################### w=3; k=7; hidden=6; nl=1 ###########################\n",
      " 500: train loss: 0.790; valid loss: 0.400\n",
      "1000: train loss: 0.720; valid loss: 0.361\n",
      "1500: train loss: 0.688; valid loss: 0.347\n",
      "2000: train loss: 0.672; valid loss: 0.339\n",
      "raw_ts: 592.434\n",
      "hidden_ts: 366.492\n",
      "Raw ts score: 0.682 +- 0.041\n",
      "Hidden ts score: 0.750 +- 0.048\n",
      "########################### w=3; k=7; hidden=6; nl=2 ###########################\n",
      " 500: train loss: 0.844; valid loss: 0.423\n",
      "1000: train loss: 0.737; valid loss: 0.364\n",
      "1500: train loss: 0.702; valid loss: 0.344\n",
      "2000: train loss: 0.677; valid loss: 0.336\n",
      "raw_ts: 590.243\n",
      "hidden_ts: 361.873\n",
      "Raw ts score: 0.663 +- 0.040\n",
      "Hidden ts score: 0.655 +- 0.038\n",
      "########################### w=3; k=7; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 0.838; valid loss: 0.428\n",
      "1000: train loss: 0.801; valid loss: 0.406\n",
      "1500: train loss: 0.763; valid loss: 0.396\n",
      "2000: train loss: 0.761; valid loss: 0.390\n",
      "raw_ts: 589.664\n",
      "hidden_ts: 364.553\n",
      "Raw ts score: 0.697 +- 0.036\n",
      "Hidden ts score: 0.604 +- 0.041\n",
      "########################### w=5; k=3; hidden=4; nl=1 ###########################\n",
      " 500: train loss: 0.832; valid loss: 0.414\n",
      "1000: train loss: 0.714; valid loss: 0.357\n",
      "1500: train loss: 0.685; valid loss: 0.343\n",
      "2000: train loss: 0.674; valid loss: 0.337\n",
      "raw_ts: 597.730\n",
      "hidden_ts: 156.704\n",
      "Raw ts score: 0.721 +- 0.040\n",
      "Hidden ts score: 0.679 +- 0.043\n",
      "########################### w=5; k=3; hidden=4; nl=2 ###########################\n",
      " 500: train loss: 0.995; valid loss: 0.496\n",
      "1000: train loss: 0.760; valid loss: 0.380\n",
      "1500: train loss: 0.728; valid loss: 0.362\n",
      "2000: train loss: 0.662; valid loss: 0.333\n",
      "raw_ts: 583.555\n",
      "hidden_ts: 160.698\n",
      "Raw ts score: 0.712 +- 0.041\n",
      "Hidden ts score: 0.646 +- 0.037\n",
      "########################### w=5; k=3; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 0.988; valid loss: 0.500\n",
      "1000: train loss: 0.927; valid loss: 0.462\n",
      "1500: train loss: 0.879; valid loss: 0.447\n",
      "2000: train loss: 0.870; valid loss: 0.441\n",
      "raw_ts: 594.851\n",
      "hidden_ts: 156.348\n",
      "Raw ts score: 0.668 +- 0.046\n",
      "Hidden ts score: 0.560 +- 0.043\n",
      "########################### w=5; k=3; hidden=6; nl=1 ###########################\n",
      " 500: train loss: 0.764; valid loss: 0.374\n",
      "1000: train loss: 0.689; valid loss: 0.336\n",
      "1500: train loss: 0.664; valid loss: 0.321\n",
      "2000: train loss: 0.643; valid loss: 0.314\n",
      "raw_ts: 580.460\n",
      "hidden_ts: 159.417\n",
      "Raw ts score: 0.711 +- 0.036\n",
      "Hidden ts score: 0.679 +- 0.039\n",
      "########################### w=5; k=3; hidden=6; nl=2 ###########################\n",
      " 500: train loss: 0.813; valid loss: 0.411\n",
      "1000: train loss: 0.738; valid loss: 0.370\n",
      "1500: train loss: 0.716; valid loss: 0.361\n",
      "2000: train loss: 0.710; valid loss: 0.356\n",
      "raw_ts: 616.055\n",
      "hidden_ts: 159.421\n",
      "Raw ts score: 0.692 +- 0.042\n",
      "Hidden ts score: 0.629 +- 0.041\n",
      "########################### w=5; k=3; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.011; valid loss: 0.494\n",
      "1000: train loss: 0.946; valid loss: 0.463\n",
      "1500: train loss: 0.900; valid loss: 0.445\n",
      "2000: train loss: 0.888; valid loss: 0.437\n",
      "raw_ts: 607.893\n",
      "hidden_ts: 162.919\n",
      "Raw ts score: 0.708 +- 0.040\n",
      "Hidden ts score: 0.511 +- 0.040\n",
      "########################### w=5; k=5; hidden=4; nl=1 ###########################\n",
      " 500: train loss: 0.859; valid loss: 0.437\n",
      "1000: train loss: 0.762; valid loss: 0.390\n",
      "1500: train loss: 0.715; valid loss: 0.365\n",
      "2000: train loss: 0.696; valid loss: 0.356\n",
      "raw_ts: 600.605\n",
      "hidden_ts: 161.417\n",
      "Raw ts score: 0.713 +- 0.045\n",
      "Hidden ts score: 0.639 +- 0.041\n",
      "########################### w=5; k=5; hidden=4; nl=2 ###########################\n",
      " 500: train loss: 0.820; valid loss: 0.408\n",
      "1000: train loss: 0.758; valid loss: 0.381\n",
      "1500: train loss: 0.740; valid loss: 0.372\n",
      "2000: train loss: 0.732; valid loss: 0.366\n",
      "raw_ts: 607.007\n",
      "hidden_ts: 164.651\n",
      "Raw ts score: 0.679 +- 0.042\n",
      "Hidden ts score: 0.648 +- 0.041\n",
      "########################### w=5; k=5; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 1.059; valid loss: 0.527\n",
      "1000: train loss: 0.953; valid loss: 0.473\n",
      "1500: train loss: 0.912; valid loss: 0.455\n",
      "2000: train loss: 0.888; valid loss: 0.447\n",
      "raw_ts: 604.038\n",
      "hidden_ts: 155.844\n",
      "Raw ts score: 0.641 +- 0.034\n",
      "Hidden ts score: 0.567 +- 0.038\n",
      "########################### w=5; k=5; hidden=6; nl=1 ###########################\n",
      " 500: train loss: 0.777; valid loss: 0.392\n",
      "1000: train loss: 0.709; valid loss: 0.355\n",
      "1500: train loss: 0.680; valid loss: 0.342\n",
      "2000: train loss: 0.663; valid loss: 0.333\n",
      "raw_ts: 575.145\n",
      "hidden_ts: 157.232\n",
      "Raw ts score: 0.674 +- 0.041\n",
      "Hidden ts score: 0.682 +- 0.043\n",
      "########################### w=5; k=5; hidden=6; nl=2 ###########################\n",
      " 500: train loss: 0.982; valid loss: 0.487\n",
      "1000: train loss: 0.751; valid loss: 0.374\n",
      "1500: train loss: 0.727; valid loss: 0.362\n",
      "2000: train loss: 0.701; valid loss: 0.350\n",
      "raw_ts: 591.650\n",
      "hidden_ts: 158.663\n",
      "Raw ts score: 0.637 +- 0.040\n",
      "Hidden ts score: 0.621 +- 0.042\n",
      "########################### w=5; k=5; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 0.989; valid loss: 0.496\n",
      "1000: train loss: 0.943; valid loss: 0.481\n",
      "1500: train loss: 0.914; valid loss: 0.472\n",
      "2000: train loss: 0.892; valid loss: 0.460\n",
      "raw_ts: 589.953\n",
      "hidden_ts: 159.170\n",
      "Raw ts score: 0.686 +- 0.044\n",
      "Hidden ts score: 0.525 +- 0.050\n",
      "########################### w=5; k=7; hidden=4; nl=1 ###########################\n",
      " 500: train loss: 0.874; valid loss: 0.435\n",
      "1000: train loss: 0.792; valid loss: 0.395\n",
      "1500: train loss: 0.744; valid loss: 0.374\n",
      "2000: train loss: 0.733; valid loss: 0.364\n",
      "raw_ts: 604.117\n",
      "hidden_ts: 140.196\n",
      "Raw ts score: 0.681 +- 0.048\n",
      "Hidden ts score: 0.630 +- 0.042\n",
      "########################### w=5; k=7; hidden=4; nl=2 ###########################\n",
      " 500: train loss: 0.868; valid loss: 0.433\n",
      "1000: train loss: 0.745; valid loss: 0.369\n",
      "1500: train loss: 0.700; valid loss: 0.350\n",
      "2000: train loss: 0.675; valid loss: 0.342\n",
      "raw_ts: 590.828\n",
      "hidden_ts: 144.702\n",
      "Raw ts score: 0.669 +- 0.045\n",
      "Hidden ts score: 0.615 +- 0.037\n",
      "########################### w=5; k=7; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 0.985; valid loss: 0.487\n",
      "1000: train loss: 0.912; valid loss: 0.460\n",
      "1500: train loss: 0.878; valid loss: 0.441\n",
      "2000: train loss: 0.860; valid loss: 0.433\n",
      "raw_ts: 580.932\n",
      "hidden_ts: 142.124\n",
      "Raw ts score: 0.677 +- 0.039\n",
      "Hidden ts score: 0.570 +- 0.038\n",
      "########################### w=5; k=7; hidden=6; nl=1 ###########################\n",
      " 500: train loss: 0.820; valid loss: 0.410\n",
      "1000: train loss: 0.737; valid loss: 0.368\n",
      "1500: train loss: 0.706; valid loss: 0.357\n",
      "2000: train loss: 0.695; valid loss: 0.349\n",
      "raw_ts: 589.382\n",
      "hidden_ts: 139.078\n",
      "Raw ts score: 0.707 +- 0.035\n",
      "Hidden ts score: 0.654 +- 0.037\n",
      "########################### w=5; k=7; hidden=6; nl=2 ###########################\n",
      " 500: train loss: 0.828; valid loss: 0.420\n",
      "1000: train loss: 0.766; valid loss: 0.383\n",
      "1500: train loss: 0.717; valid loss: 0.359\n",
      "2000: train loss: 0.688; valid loss: 0.347\n",
      "raw_ts: 577.240\n",
      "hidden_ts: 143.084\n",
      "Raw ts score: 0.710 +- 0.037\n",
      "Hidden ts score: 0.701 +- 0.039\n",
      "########################### w=5; k=7; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 0.965; valid loss: 0.490\n",
      "1000: train loss: 0.931; valid loss: 0.471\n",
      "1500: train loss: 0.886; valid loss: 0.457\n",
      "2000: train loss: 0.861; valid loss: 0.447\n",
      "raw_ts: 592.512\n",
      "hidden_ts: 140.809\n",
      "Raw ts score: 0.709 +- 0.040\n",
      "Hidden ts score: 0.593 +- 0.035\n",
      "########################### w=7; k=3; hidden=4; nl=1 ###########################\n",
      " 500: train loss: 0.908; valid loss: 0.460\n",
      "1000: train loss: 0.755; valid loss: 0.383\n",
      "1500: train loss: 0.709; valid loss: 0.361\n",
      "2000: train loss: 0.672; valid loss: 0.346\n",
      "raw_ts: 588.693\n",
      "hidden_ts: 86.525\n",
      "Raw ts score: 0.677 +- 0.036\n",
      "Hidden ts score: 0.602 +- 0.042\n",
      "########################### w=7; k=3; hidden=4; nl=2 ###########################\n",
      " 500: train loss: 1.096; valid loss: 0.546\n",
      "1000: train loss: 0.891; valid loss: 0.442\n",
      "1500: train loss: 0.706; valid loss: 0.352\n",
      "2000: train loss: 0.678; valid loss: 0.341\n",
      "raw_ts: 589.804\n",
      "hidden_ts: 84.944\n",
      "Raw ts score: 0.661 +- 0.042\n",
      "Hidden ts score: 0.537 +- 0.045\n",
      "########################### w=7; k=3; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 1.259; valid loss: 0.623\n",
      "1000: train loss: 1.151; valid loss: 0.579\n",
      "1500: train loss: 1.035; valid loss: 0.518\n",
      "2000: train loss: 0.989; valid loss: 0.499\n",
      "raw_ts: 574.091\n",
      "hidden_ts: 84.433\n",
      "Raw ts score: 0.714 +- 0.037\n",
      "Hidden ts score: 0.527 +- 0.037\n",
      "########################### w=7; k=3; hidden=6; nl=1 ###########################\n",
      " 500: train loss: 0.803; valid loss: 0.404\n",
      "1000: train loss: 0.719; valid loss: 0.356\n",
      "1500: train loss: 0.672; valid loss: 0.338\n",
      "2000: train loss: 0.660; valid loss: 0.332\n",
      "raw_ts: 604.256\n",
      "hidden_ts: 85.130\n",
      "Raw ts score: 0.660 +- 0.041\n",
      "Hidden ts score: 0.568 +- 0.042\n",
      "########################### w=7; k=3; hidden=6; nl=2 ###########################\n",
      " 500: train loss: 0.851; valid loss: 0.421\n",
      "1000: train loss: 0.800; valid loss: 0.390\n",
      "1500: train loss: 0.769; valid loss: 0.380\n",
      "2000: train loss: 0.706; valid loss: 0.345\n",
      "raw_ts: 591.053\n",
      "hidden_ts: 86.885\n",
      "Raw ts score: 0.683 +- 0.032\n",
      "Hidden ts score: 0.611 +- 0.042\n",
      "########################### w=7; k=3; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.106; valid loss: 0.551\n",
      "1000: train loss: 0.988; valid loss: 0.501\n",
      "1500: train loss: 0.951; valid loss: 0.483\n",
      "2000: train loss: 0.935; valid loss: 0.472\n",
      "raw_ts: 574.776\n",
      "hidden_ts: 86.525\n",
      "Raw ts score: 0.726 +- 0.043\n",
      "Hidden ts score: 0.399 +- 0.040\n",
      "########################### w=7; k=5; hidden=4; nl=1 ###########################\n",
      " 500: train loss: 0.885; valid loss: 0.450\n",
      "1000: train loss: 0.781; valid loss: 0.396\n",
      "1500: train loss: 0.747; valid loss: 0.376\n",
      "2000: train loss: 0.719; valid loss: 0.366\n",
      "raw_ts: 575.973\n",
      "hidden_ts: 73.789\n",
      "Raw ts score: 0.674 +- 0.041\n",
      "Hidden ts score: 0.588 +- 0.042\n",
      "########################### w=7; k=5; hidden=4; nl=2 ###########################\n",
      " 500: train loss: 0.911; valid loss: 0.453\n",
      "1000: train loss: 0.769; valid loss: 0.380\n",
      "1500: train loss: 0.742; valid loss: 0.369\n",
      "2000: train loss: 0.725; valid loss: 0.364\n",
      "raw_ts: 573.655\n",
      "hidden_ts: 73.553\n",
      "Raw ts score: 0.663 +- 0.044\n",
      "Hidden ts score: 0.598 +- 0.049\n",
      "########################### w=7; k=5; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 1.112; valid loss: 0.557\n",
      "1000: train loss: 1.009; valid loss: 0.516\n",
      "1500: train loss: 0.982; valid loss: 0.505\n",
      "2000: train loss: 0.965; valid loss: 0.497\n",
      "raw_ts: 591.961\n",
      "hidden_ts: 75.221\n",
      "Raw ts score: 0.673 +- 0.039\n",
      "Hidden ts score: 0.515 +- 0.040\n",
      "########################### w=7; k=5; hidden=6; nl=1 ###########################\n",
      " 500: train loss: 0.828; valid loss: 0.407\n",
      "1000: train loss: 0.739; valid loss: 0.364\n",
      "1500: train loss: 0.704; valid loss: 0.348\n",
      "2000: train loss: 0.684; valid loss: 0.341\n",
      "raw_ts: 587.825\n",
      "hidden_ts: 73.555\n",
      "Raw ts score: 0.697 +- 0.034\n",
      "Hidden ts score: 0.551 +- 0.046\n",
      "########################### w=7; k=5; hidden=6; nl=2 ###########################\n",
      " 500: train loss: 0.874; valid loss: 0.440\n",
      "1000: train loss: 0.732; valid loss: 0.368\n",
      "1500: train loss: 0.684; valid loss: 0.344\n",
      "2000: train loss: 0.678; valid loss: 0.339\n",
      "raw_ts: 575.137\n",
      "hidden_ts: 72.609\n",
      "Raw ts score: 0.721 +- 0.041\n",
      "Hidden ts score: 0.593 +- 0.044\n",
      "########################### w=7; k=5; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.081; valid loss: 0.536\n",
      "1000: train loss: 1.015; valid loss: 0.504\n",
      "1500: train loss: 0.965; valid loss: 0.487\n",
      "2000: train loss: 0.942; valid loss: 0.478\n",
      "raw_ts: 587.333\n",
      "hidden_ts: 73.527\n",
      "Raw ts score: 0.657 +- 0.048\n",
      "Hidden ts score: 0.542 +- 0.042\n",
      "########################### w=7; k=7; hidden=4; nl=1 ###########################\n",
      " 500: train loss: 0.958; valid loss: 0.475\n",
      "1000: train loss: 0.814; valid loss: 0.400\n",
      "1500: train loss: 0.769; valid loss: 0.382\n",
      "2000: train loss: 0.755; valid loss: 0.374\n",
      "raw_ts: 588.708\n",
      "hidden_ts: 73.091\n",
      "Raw ts score: 0.694 +- 0.042\n",
      "Hidden ts score: 0.609 +- 0.049\n",
      "########################### w=7; k=7; hidden=4; nl=2 ###########################\n",
      " 500: train loss: 0.992; valid loss: 0.501\n",
      "1000: train loss: 0.852; valid loss: 0.437\n",
      "1500: train loss: 0.753; valid loss: 0.386\n",
      "2000: train loss: 0.737; valid loss: 0.379\n",
      "raw_ts: 592.275\n",
      "hidden_ts: 73.972\n",
      "Raw ts score: 0.722 +- 0.033\n",
      "Hidden ts score: 0.532 +- 0.045\n",
      "########################### w=7; k=7; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 1.140; valid loss: 0.585\n",
      "1000: train loss: 1.050; valid loss: 0.538\n",
      "1500: train loss: 1.008; valid loss: 0.522\n",
      "2000: train loss: 0.991; valid loss: 0.514\n",
      "raw_ts: 589.829\n",
      "hidden_ts: 72.220\n",
      "Raw ts score: 0.710 +- 0.035\n",
      "Hidden ts score: 0.477 +- 0.047\n",
      "########################### w=7; k=7; hidden=6; nl=1 ###########################\n",
      " 500: train loss: 0.846; valid loss: 0.416\n",
      "1000: train loss: 0.766; valid loss: 0.380\n",
      "1500: train loss: 0.727; valid loss: 0.358\n",
      "2000: train loss: 0.703; valid loss: 0.350\n",
      "raw_ts: 584.479\n",
      "hidden_ts: 73.326\n",
      "Raw ts score: 0.684 +- 0.043\n",
      "Hidden ts score: 0.569 +- 0.051\n",
      "########################### w=7; k=7; hidden=6; nl=2 ###########################\n",
      " 500: train loss: 0.822; valid loss: 0.409\n",
      "1000: train loss: 0.759; valid loss: 0.381\n",
      "1500: train loss: 0.715; valid loss: 0.360\n",
      "2000: train loss: 0.681; valid loss: 0.343\n",
      "raw_ts: 593.062\n",
      "hidden_ts: 73.484\n",
      "Raw ts score: 0.713 +- 0.042\n",
      "Hidden ts score: 0.591 +- 0.043\n",
      "########################### w=7; k=7; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.089; valid loss: 0.548\n",
      "1000: train loss: 1.002; valid loss: 0.509\n",
      "1500: train loss: 0.945; valid loss: 0.494\n",
      "2000: train loss: 0.943; valid loss: 0.486\n",
      "raw_ts: 575.216\n",
      "hidden_ts: 75.148\n",
      "Raw ts score: 0.698 +- 0.040\n",
      "Hidden ts score: 0.465 +- 0.046\n",
      "Raw ts score: 0.687 +- 0.047\n"
     ]
    }
   ],
   "source": [
    "LENGTHS = [100]\n",
    "W = [3, 5, 7]\n",
    "K = [3, 5, 7]\n",
    "HIDDEN_DIM = [4, 6]\n",
    "N_LAYERS = [1, 2, 4]\n",
    "raw_scores = []\n",
    "for w in W:\n",
    "    for k in K:\n",
    "        for length in LENGTHS:\n",
    "            for hidden_dim in HIDDEN_DIM:\n",
    "                for n_layers in N_LAYERS:\n",
    "                    print(\"{:#^80}\".format(\" w={}; k={}; hidden={}; nl={} \".format(w, k, hidden_dim, n_layers)))\n",
    "                    \n",
    "                    raw_scores += workflow(w, k, hidden_dim, n_layers, length, 2000)\n",
    "\n",
    "print(\"Raw ts score: {:.3f} +- {:.3f}\".format(np.mean(raw_scores), np.std(raw_scores)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################### w=3; k=3; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 1.321; valid loss: 0.653\n",
      "1000: train loss: 0.826; valid loss: 0.414\n",
      "1500: train loss: 0.806; valid loss: 0.401\n",
      "2000: train loss: 0.791; valid loss: 0.394\n",
      "2500: train loss: 0.775; valid loss: 0.387\n",
      "3000: train loss: 0.756; valid loss: 0.382\n",
      "3500: train loss: 0.753; valid loss: 0.379\n",
      "4000: train loss: 0.755; valid loss: 0.377\n",
      "raw_ts: 582.760\n",
      "hidden_ts: 437.862\n",
      "Raw ts score: 0.699 +- 0.037\n",
      "Hidden ts score: 0.570 +- 0.048\n",
      "########################### w=3; k=3; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.337; valid loss: 0.672\n",
      "1000: train loss: 0.790; valid loss: 0.395\n",
      "1500: train loss: 0.772; valid loss: 0.379\n",
      "2000: train loss: 0.721; valid loss: 0.357\n",
      "2500: train loss: 0.654; valid loss: 0.324\n",
      "3000: train loss: 0.643; valid loss: 0.316\n",
      "3500: train loss: 0.638; valid loss: 0.313\n",
      "4000: train loss: 0.636; valid loss: 0.311\n",
      "raw_ts: 588.209\n",
      "hidden_ts: 442.137\n",
      "Raw ts score: 0.674 +- 0.043\n",
      "Hidden ts score: 0.669 +- 0.046\n",
      "########################## w=3; k=3; hidden=10; nl=4 ###########################\n",
      " 500: train loss: 0.918; valid loss: 0.464\n",
      "1000: train loss: 0.770; valid loss: 0.397\n",
      "1500: train loss: 0.757; valid loss: 0.386\n",
      "2000: train loss: 0.740; valid loss: 0.380\n",
      "2500: train loss: 0.738; valid loss: 0.375\n",
      "3000: train loss: 0.721; valid loss: 0.371\n",
      "3500: train loss: 0.703; valid loss: 0.368\n",
      "4000: train loss: 0.707; valid loss: 0.364\n",
      "raw_ts: 605.435\n",
      "hidden_ts: 441.809\n",
      "Raw ts score: 0.699 +- 0.039\n",
      "Hidden ts score: 0.640 +- 0.043\n",
      "########################### w=3; k=5; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 0.864; valid loss: 0.424\n",
      "1000: train loss: 0.822; valid loss: 0.400\n",
      "1500: train loss: 0.796; valid loss: 0.387\n",
      "2000: train loss: 0.770; valid loss: 0.380\n",
      "2500: train loss: 0.770; valid loss: 0.376\n",
      "3000: train loss: 0.757; valid loss: 0.373\n",
      "3500: train loss: 0.750; valid loss: 0.371\n",
      "4000: train loss: 0.752; valid loss: 0.369\n",
      "raw_ts: 606.597\n",
      "hidden_ts: 412.415\n",
      "Raw ts score: 0.716 +- 0.034\n",
      "Hidden ts score: 0.593 +- 0.041\n",
      "########################### w=3; k=5; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 0.874; valid loss: 0.435\n",
      "1000: train loss: 0.806; valid loss: 0.403\n",
      "1500: train loss: 0.787; valid loss: 0.392\n",
      "2000: train loss: 0.766; valid loss: 0.384\n",
      "2500: train loss: 0.756; valid loss: 0.379\n",
      "3000: train loss: 0.750; valid loss: 0.376\n",
      "3500: train loss: 0.742; valid loss: 0.374\n",
      "4000: train loss: 0.727; valid loss: 0.372\n",
      "raw_ts: 575.350\n",
      "hidden_ts: 415.816\n",
      "Raw ts score: 0.642 +- 0.039\n",
      "Hidden ts score: 0.514 +- 0.043\n",
      "########################## w=3; k=5; hidden=10; nl=4 ###########################\n",
      " 500: train loss: 0.827; valid loss: 0.420\n",
      "1000: train loss: 0.784; valid loss: 0.399\n",
      "1500: train loss: 0.765; valid loss: 0.388\n",
      "2000: train loss: 0.748; valid loss: 0.383\n",
      "2500: train loss: 0.736; valid loss: 0.379\n",
      "3000: train loss: 0.740; valid loss: 0.376\n",
      "3500: train loss: 0.727; valid loss: 0.372\n",
      "4000: train loss: 0.724; valid loss: 0.369\n",
      "raw_ts: 589.147\n",
      "hidden_ts: 411.206\n",
      "Raw ts score: 0.662 +- 0.038\n",
      "Hidden ts score: 0.542 +- 0.043\n",
      "########################### w=3; k=7; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 0.863; valid loss: 0.427\n",
      "1000: train loss: 0.793; valid loss: 0.403\n",
      "1500: train loss: 0.779; valid loss: 0.395\n",
      "2000: train loss: 0.767; valid loss: 0.389\n",
      "2500: train loss: 0.757; valid loss: 0.386\n",
      "3000: train loss: 0.746; valid loss: 0.384\n",
      "3500: train loss: 0.755; valid loss: 0.382\n",
      "4000: train loss: 0.746; valid loss: 0.381\n",
      "raw_ts: 588.520\n",
      "hidden_ts: 368.658\n",
      "Raw ts score: 0.701 +- 0.040\n",
      "Hidden ts score: 0.587 +- 0.041\n",
      "########################### w=3; k=7; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 0.898; valid loss: 0.446\n",
      "1000: train loss: 0.816; valid loss: 0.406\n",
      "1500: train loss: 0.791; valid loss: 0.395\n",
      "2000: train loss: 0.776; valid loss: 0.388\n",
      "2500: train loss: 0.776; valid loss: 0.383\n",
      "3000: train loss: 0.750; valid loss: 0.380\n",
      "3500: train loss: 0.754; valid loss: 0.377\n",
      "4000: train loss: 0.743; valid loss: 0.374\n",
      "raw_ts: 593.728\n",
      "hidden_ts: 362.019\n",
      "Raw ts score: 0.674 +- 0.043\n",
      "Hidden ts score: 0.537 +- 0.042\n",
      "########################## w=3; k=7; hidden=10; nl=4 ###########################\n",
      " 500: train loss: 0.849; valid loss: 0.420\n",
      "1000: train loss: 0.790; valid loss: 0.399\n",
      "1500: train loss: 0.762; valid loss: 0.388\n",
      "2000: train loss: 0.754; valid loss: 0.381\n",
      "2500: train loss: 0.732; valid loss: 0.375\n",
      "3000: train loss: 0.726; valid loss: 0.365\n",
      "3500: train loss: 0.678; valid loss: 0.343\n",
      "4000: train loss: 0.658; valid loss: 0.331\n",
      "raw_ts: 590.448\n",
      "hidden_ts: 377.362\n",
      "Raw ts score: 0.689 +- 0.046\n",
      "Hidden ts score: 0.645 +- 0.041\n",
      "########################### w=5; k=3; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 1.067; valid loss: 0.524\n",
      "1000: train loss: 0.946; valid loss: 0.467\n",
      "1500: train loss: 0.882; valid loss: 0.433\n",
      "2000: train loss: 0.856; valid loss: 0.416\n",
      "2500: train loss: 0.830; valid loss: 0.408\n",
      "3000: train loss: 0.829; valid loss: 0.402\n",
      "3500: train loss: 0.824; valid loss: 0.398\n",
      "4000: train loss: 0.800; valid loss: 0.394\n",
      "raw_ts: 574.719\n",
      "hidden_ts: 155.215\n",
      "Raw ts score: 0.712 +- 0.043\n",
      "Hidden ts score: 0.352 +- 0.042\n",
      "########################### w=5; k=3; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 0.980; valid loss: 0.490\n",
      "1000: train loss: 0.934; valid loss: 0.466\n",
      "1500: train loss: 0.901; valid loss: 0.449\n",
      "2000: train loss: 0.882; valid loss: 0.438\n",
      "2500: train loss: 0.855; valid loss: 0.431\n",
      "3000: train loss: 0.847; valid loss: 0.426\n",
      "3500: train loss: 0.840; valid loss: 0.422\n",
      "4000: train loss: 0.819; valid loss: 0.418\n",
      "raw_ts: 582.189\n",
      "hidden_ts: 161.251\n",
      "Raw ts score: 0.703 +- 0.043\n",
      "Hidden ts score: 0.604 +- 0.045\n",
      "########################## w=5; k=3; hidden=10; nl=4 ###########################\n",
      " 500: train loss: 0.970; valid loss: 0.495\n",
      "1000: train loss: 0.908; valid loss: 0.458\n",
      "1500: train loss: 0.847; valid loss: 0.435\n",
      "2000: train loss: 0.821; valid loss: 0.424\n",
      "2500: train loss: 0.807; valid loss: 0.416\n",
      "3000: train loss: 0.788; valid loss: 0.406\n",
      "3500: train loss: 0.743; valid loss: 0.380\n",
      "4000: train loss: 0.717; valid loss: 0.367\n",
      "raw_ts: 603.316\n",
      "hidden_ts: 158.807\n",
      "Raw ts score: 0.713 +- 0.040\n",
      "Hidden ts score: 0.549 +- 0.046\n",
      "########################### w=5; k=5; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 0.991; valid loss: 0.503\n",
      "1000: train loss: 0.960; valid loss: 0.480\n",
      "1500: train loss: 0.951; valid loss: 0.469\n",
      "2000: train loss: 0.901; valid loss: 0.459\n",
      "2500: train loss: 0.897; valid loss: 0.451\n",
      "3000: train loss: 0.860; valid loss: 0.442\n",
      "3500: train loss: 0.845; valid loss: 0.437\n",
      "4000: train loss: 0.791; valid loss: 0.401\n",
      "raw_ts: 586.445\n",
      "hidden_ts: 162.966\n",
      "Raw ts score: 0.664 +- 0.038\n",
      "Hidden ts score: 0.521 +- 0.044\n",
      "########################### w=5; k=5; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.047; valid loss: 0.521\n",
      "1000: train loss: 0.971; valid loss: 0.482\n",
      "1500: train loss: 0.912; valid loss: 0.455\n",
      "2000: train loss: 0.878; valid loss: 0.439\n",
      "2500: train loss: 0.851; valid loss: 0.429\n",
      "3000: train loss: 0.826; valid loss: 0.413\n",
      "3500: train loss: 0.750; valid loss: 0.372\n",
      "4000: train loss: 0.735; valid loss: 0.365\n",
      "raw_ts: 600.692\n",
      "hidden_ts: 156.529\n",
      "Raw ts score: 0.687 +- 0.037\n",
      "Hidden ts score: 0.521 +- 0.039\n",
      "########################## w=5; k=5; hidden=10; nl=4 ###########################\n",
      " 500: train loss: 0.973; valid loss: 0.487\n",
      "1000: train loss: 0.916; valid loss: 0.456\n",
      "1500: train loss: 0.857; valid loss: 0.434\n",
      "2000: train loss: 0.793; valid loss: 0.402\n",
      "2500: train loss: 0.763; valid loss: 0.380\n",
      "3000: train loss: 0.737; valid loss: 0.370\n",
      "3500: train loss: 0.718; valid loss: 0.362\n",
      "4000: train loss: 0.706; valid loss: 0.357\n",
      "raw_ts: 574.202\n",
      "hidden_ts: 157.199\n",
      "Raw ts score: 0.701 +- 0.041\n",
      "Hidden ts score: 0.563 +- 0.045\n",
      "########################### w=5; k=7; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 0.993; valid loss: 0.500\n",
      "1000: train loss: 0.938; valid loss: 0.475\n",
      "1500: train loss: 0.922; valid loss: 0.462\n",
      "2000: train loss: 0.879; valid loss: 0.455\n",
      "2500: train loss: 0.890; valid loss: 0.450\n",
      "3000: train loss: 0.862; valid loss: 0.445\n",
      "3500: train loss: 0.791; valid loss: 0.397\n",
      "4000: train loss: 0.760; valid loss: 0.385\n",
      "raw_ts: 606.781\n",
      "hidden_ts: 142.386\n",
      "Raw ts score: 0.662 +- 0.047\n",
      "Hidden ts score: 0.573 +- 0.050\n",
      "########################### w=5; k=7; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.040; valid loss: 0.521\n",
      "1000: train loss: 0.882; valid loss: 0.440\n",
      "1500: train loss: 0.836; valid loss: 0.422\n",
      "2000: train loss: 0.823; valid loss: 0.414\n",
      "2500: train loss: 0.771; valid loss: 0.386\n",
      "3000: train loss: 0.737; valid loss: 0.369\n",
      "3500: train loss: 0.723; valid loss: 0.362\n",
      "4000: train loss: 0.709; valid loss: 0.358\n",
      "raw_ts: 604.723\n",
      "hidden_ts: 139.822\n",
      "Raw ts score: 0.685 +- 0.043\n",
      "Hidden ts score: 0.606 +- 0.045\n",
      "########################## w=5; k=7; hidden=10; nl=4 ###########################\n",
      " 500: train loss: 0.954; valid loss: 0.479\n",
      "1000: train loss: 0.892; valid loss: 0.452\n",
      "1500: train loss: 0.857; valid loss: 0.431\n",
      "2000: train loss: 0.777; valid loss: 0.386\n",
      "2500: train loss: 0.739; valid loss: 0.367\n",
      "3000: train loss: 0.730; valid loss: 0.361\n",
      "3500: train loss: 0.708; valid loss: 0.357\n",
      "4000: train loss: 0.706; valid loss: 0.354\n",
      "raw_ts: 592.416\n",
      "hidden_ts: 140.463\n",
      "Raw ts score: 0.673 +- 0.041\n",
      "Hidden ts score: 0.583 +- 0.039\n",
      "########################### w=7; k=3; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 1.133; valid loss: 0.571\n",
      "1000: train loss: 0.972; valid loss: 0.484\n",
      "1500: train loss: 0.906; valid loss: 0.455\n",
      "2000: train loss: 0.878; valid loss: 0.438\n",
      "2500: train loss: 0.859; valid loss: 0.428\n",
      "3000: train loss: 0.856; valid loss: 0.423\n",
      "3500: train loss: 0.837; valid loss: 0.420\n",
      "4000: train loss: 0.835; valid loss: 0.417\n",
      "raw_ts: 593.549\n",
      "hidden_ts: 86.639\n",
      "Raw ts score: 0.681 +- 0.039\n",
      "Hidden ts score: 0.519 +- 0.046\n",
      "########################### w=7; k=3; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.087; valid loss: 0.546\n",
      "1000: train loss: 1.010; valid loss: 0.510\n",
      "1500: train loss: 0.967; valid loss: 0.494\n",
      "2000: train loss: 0.953; valid loss: 0.484\n",
      "2500: train loss: 0.938; valid loss: 0.476\n",
      "3000: train loss: 0.908; valid loss: 0.466\n",
      "3500: train loss: 0.880; valid loss: 0.447\n",
      "4000: train loss: 0.830; valid loss: 0.421\n",
      "raw_ts: 593.139\n",
      "hidden_ts: 87.815\n",
      "Raw ts score: 0.666 +- 0.040\n",
      "Hidden ts score: 0.503 +- 0.042\n",
      "########################## w=7; k=3; hidden=10; nl=4 ###########################\n",
      " 500: train loss: 1.057; valid loss: 0.527\n",
      "1000: train loss: 0.983; valid loss: 0.496\n",
      "1500: train loss: 0.944; valid loss: 0.478\n",
      "2000: train loss: 0.929; valid loss: 0.469\n",
      "2500: train loss: 0.914; valid loss: 0.458\n",
      "3000: train loss: 0.856; valid loss: 0.432\n",
      "3500: train loss: 0.821; valid loss: 0.414\n",
      "4000: train loss: 0.801; valid loss: 0.408\n",
      "raw_ts: 616.490\n",
      "hidden_ts: 87.051\n",
      "Raw ts score: 0.745 +- 0.035\n",
      "Hidden ts score: 0.523 +- 0.044\n",
      "########################### w=7; k=5; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 1.077; valid loss: 0.540\n",
      "1000: train loss: 0.994; valid loss: 0.500\n",
      "1500: train loss: 0.963; valid loss: 0.486\n",
      "2000: train loss: 0.954; valid loss: 0.479\n",
      "2500: train loss: 0.962; valid loss: 0.474\n",
      "3000: train loss: 0.926; valid loss: 0.467\n",
      "3500: train loss: 0.914; valid loss: 0.457\n",
      "4000: train loss: 0.858; valid loss: 0.432\n",
      "raw_ts: 589.663\n",
      "hidden_ts: 73.891\n",
      "Raw ts score: 0.666 +- 0.035\n",
      "Hidden ts score: 0.401 +- 0.038\n",
      "########################### w=7; k=5; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.128; valid loss: 0.569\n",
      "1000: train loss: 1.054; valid loss: 0.523\n",
      "1500: train loss: 0.879; valid loss: 0.436\n",
      "2000: train loss: 0.824; valid loss: 0.412\n",
      "2500: train loss: 0.787; valid loss: 0.403\n",
      "3000: train loss: 0.779; valid loss: 0.398\n",
      "3500: train loss: 0.768; valid loss: 0.394\n",
      "4000: train loss: 0.773; valid loss: 0.391\n",
      "raw_ts: 589.577\n",
      "hidden_ts: 72.955\n",
      "Raw ts score: 0.675 +- 0.044\n",
      "Hidden ts score: 0.506 +- 0.037\n",
      "########################## w=7; k=5; hidden=10; nl=4 ###########################\n",
      " 500: train loss: 1.077; valid loss: 0.540\n",
      "1000: train loss: 1.004; valid loss: 0.509\n",
      "1500: train loss: 0.966; valid loss: 0.494\n",
      "2000: train loss: 0.944; valid loss: 0.480\n",
      "2500: train loss: 0.841; valid loss: 0.422\n",
      "3000: train loss: 0.797; valid loss: 0.398\n",
      "3500: train loss: 0.773; valid loss: 0.390\n",
      "4000: train loss: 0.763; valid loss: 0.386\n",
      "raw_ts: 589.186\n",
      "hidden_ts: 74.970\n",
      "Raw ts score: 0.663 +- 0.043\n",
      "Hidden ts score: 0.508 +- 0.040\n",
      "########################### w=7; k=7; hidden=4; nl=4 ###########################\n",
      " 500: train loss: 1.140; valid loss: 0.574\n",
      "1000: train loss: 0.986; valid loss: 0.507\n",
      "1500: train loss: 0.957; valid loss: 0.492\n",
      "2000: train loss: 0.948; valid loss: 0.487\n",
      "2500: train loss: 0.943; valid loss: 0.483\n",
      "3000: train loss: 0.924; valid loss: 0.480\n",
      "3500: train loss: 0.925; valid loss: 0.477\n",
      "4000: train loss: 0.916; valid loss: 0.475\n",
      "raw_ts: 591.428\n",
      "hidden_ts: 75.414\n",
      "Raw ts score: 0.664 +- 0.041\n",
      "Hidden ts score: 0.465 +- 0.041\n",
      "########################### w=7; k=7; hidden=6; nl=4 ###########################\n",
      " 500: train loss: 1.110; valid loss: 0.569\n",
      "1000: train loss: 1.055; valid loss: 0.544\n",
      "1500: train loss: 1.025; valid loss: 0.530\n",
      "2000: train loss: 1.003; valid loss: 0.522\n",
      "2500: train loss: 0.984; valid loss: 0.515\n",
      "3000: train loss: 0.975; valid loss: 0.511\n",
      "3500: train loss: 0.963; valid loss: 0.507\n",
      "4000: train loss: 0.954; valid loss: 0.505\n",
      "raw_ts: 597.117\n",
      "hidden_ts: 73.565\n",
      "Raw ts score: 0.680 +- 0.042\n",
      "Hidden ts score: 0.456 +- 0.047\n",
      "########################## w=7; k=7; hidden=10; nl=4 ###########################\n",
      " 500: train loss: 1.045; valid loss: 0.544\n",
      "1000: train loss: 0.976; valid loss: 0.510\n",
      "1500: train loss: 0.951; valid loss: 0.495\n",
      "2000: train loss: 0.899; valid loss: 0.476\n",
      "2500: train loss: 0.853; valid loss: 0.446\n",
      "3000: train loss: 0.808; valid loss: 0.421\n",
      "3500: train loss: 0.790; valid loss: 0.412\n",
      "4000: train loss: 0.776; valid loss: 0.405\n",
      "raw_ts: 609.949\n",
      "hidden_ts: 74.195\n",
      "Raw ts score: 0.688 +- 0.037\n",
      "Hidden ts score: 0.553 +- 0.038\n",
      "Raw ts score: 0.685 +- 0.046\n"
     ]
    }
   ],
   "source": [
    "LENGTHS = [100]\n",
    "W = [3, 5, 7]\n",
    "K = [3, 5, 7]\n",
    "HIDDEN_DIM = [4, 6, 10]\n",
    "N_LAYERS = [4]\n",
    "raw_scores = []\n",
    "for w in W:\n",
    "    for k in K:\n",
    "        for length in LENGTHS:\n",
    "            for hidden_dim in HIDDEN_DIM:\n",
    "                for n_layers in N_LAYERS:\n",
    "                    print(\"{:#^80}\".format(\" w={}; k={}; hidden={}; nl={} \".format(w, k, hidden_dim, n_layers)))\n",
    "                    \n",
    "                    raw_scores += workflow(w, k, hidden_dim, n_layers, length, 4000)\n",
    "\n",
    "print(\"Raw ts score: {:.3f} +- {:.3f}\".format(np.mean(raw_scores), np.std(raw_scores)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See decodered peaces of timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id0 = 5\n",
    "id1 = 5\n",
    "w = 3\n",
    "k = 5\n",
    "hidden_dim = 6\n",
    "length = 100\n",
    "X, y = get_dataset_2(length)\n",
    "n_layers = 1\n",
    "train_set, test_set, valid_set = prepare_data(X, y, k, w)\n",
    "model = get_model(hidden_dim, k, n_layers)\n",
    "valid_it = iter(valid_set)\n",
    "batch, timeseries, labels = next(valid_it)\n",
    "timeseries = timeseries.numpy()\n",
    "model.load_state_dict(torch.load(\"../data/w={};k={};nl={};length={}\".format(w, k, n_layers, length)))\n",
    "\n",
    "x_out = model(batch.permute(1, 0, 2)).permute(1, 0, 2).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x143522e3e48>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(batch[id0, id1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14314af5e08>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_out[id0, id1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on raw time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "matrix_ts = dtw.distance_matrix(timeseries, use_c=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(metric=get_metric(matrix_ts), algorithm=\"brute\")\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden states classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hiddens = model.encoder(batch.permute(1, 0, 2))[0].permute(1, 0, 2).detach().cpu().numpy()\n",
    "matrix_hidden = dtw_ndim.distance_matrix(hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(metric=get_metric(matrix_hidden), algorithm=\"brute\")\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ts = []\n",
    "scores_hidden = []\n",
    "\n",
    "for i in range(1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(idxs, labels.cpu().numpy(), test_size=0.3)\n",
    "\n",
    "    clf = KNeighborsClassifier(metric=get_metric(matrix_ts), algorithm=\"brute\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    scores_ts.append(score)\n",
    "    clf = KNeighborsClassifier(metric=get_metric(matrix_hidden), algorithm=\"brute\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    scores_hidden.append(score)\n",
    "    \n",
    "print(\"{:.3f}\".format(np.mean(scores_ts)))    \n",
    "print(\"{:.3f}\".format(np.mean(scores_hidden)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hidden_dtw",
   "language": "python",
   "name": "hidden_dtw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
